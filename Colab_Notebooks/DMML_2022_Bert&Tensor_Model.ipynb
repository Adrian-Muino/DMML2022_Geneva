{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrian-Muino/DMML2022_Geneva/blob/main/Colab_Notebooks/DMML_2022_Bert%26Tensor_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed = 0\n",
        "\n",
        "def evaluate(y_true, pred):\n",
        "  \"\"\"\n",
        "  This method calculates the model performance metrics. Since it is a multi-class\n",
        "  classification, we decided to take the weighted average \n",
        "  for the metrics that are calculated for each class.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  report = {\n",
        "      'accuracy':accuracy_score(y_true, pred),\n",
        "      'recall':recall_score(y_true, pred, average='macro'),\n",
        "      'precision':precision_score(y_true, pred, average='macro'),\n",
        "      'f1_score':f1_score(y_true, pred, average='macro')\n",
        "  }\n",
        "\n",
        "  return report\n",
        "\n",
        "def plot_confusion_matrix(y_true, pred, model):\n",
        "  \n",
        "  \"\"\"\n",
        "  A method plotting the models into a confusion matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  cf_matrix = confusion_matrix(y_test, pred)\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
        "                              display_labels=model.classes_)\n",
        "    \n",
        "  disp.plot()\n",
        "\n",
        "\n",
        "reports = {}\n"
      ],
      "metadata": {
        "id": "xetHYKv1Cw01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cbd427-918a-4dcc-ae31-22ab829f1799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the french spacy model\n",
        "!python -m spacy download fr_core_news_md\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbxfwUH1Y475",
        "outputId": "d68dba4c-499e-4b15-aad2-6df776a5f3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-18 21:40:32.657811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-12-18 21:40:32.657925: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-12-18 21:40:32.657946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.4.0/fr_core_news_md-3.4.0-py3-none-any.whl (45.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 45.8 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from fr-core-news-md==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-md==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_md')\n",
            "2022-12-18 21:40:50.054151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-12-18 21:40:50.054267: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-12-18 21:40:50.054287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 354 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from fr-core-news-sm==3.4.0) (3.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading in the data via the Kaggle API & mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# install Kaggle\n",
        "! pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "#read in your Kaggle credentials from Google Drive\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset from the competition page\n",
        "! kaggle competitions download -c detecting-french-texts-difficulty-level-2022\n",
        "!unzip detecting-french-texts-difficulty-level-2022.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geVeEu6DY9kL",
        "outputId": "ca0cb1a0-066d-4257-ee09-aa6f5da38820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "detecting-french-texts-difficulty-level-2022.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  detecting-french-texts-difficulty-level-2022.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"training_data.csv\", index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Nn-eJU-MYiLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "num_classes = len(df[\"difficulty\"].value_counts())\n",
        "\n",
        "colors = plt.cm.Dark2(np.linspace(0, 1, num_classes))\n",
        "iter_color = iter(colors)\n",
        "\n",
        "df['difficulty'].value_counts().plot.barh(title=\"Setences by Difficulty Level\", \n",
        "                                                 ylabel=\"Level\",\n",
        "                                                 color=colors,\n",
        "                                                 figsize=(9,9))\n",
        "\n",
        "for i, v in enumerate(df['difficulty'].value_counts()):\n",
        "  c = next(iter_color)\n",
        "  plt.text(v, i,\n",
        "           \" \"+str(v)+\", \"+str(round(v*100/df.shape[0],2))+\"%\", \n",
        "           color=c, \n",
        "           va='center', \n",
        "           fontweight='bold')"
      ],
      "metadata": {
        "id": "WLm3ujBuZmZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map topic descriptions to labels\n",
        "df['Level'] = df['difficulty'].map({'A1': 0,\n",
        "                                    'A2': 1,\n",
        "                                    'B1': 2,\n",
        "                                    'B2': 3,\n",
        "                                    'C1': 4,\n",
        "                                    'C2': 5,})\n",
        "\n",
        "# drop unused column\n",
        "df = df.drop([\"difficulty\"], axis=1)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8mFW2f3-aB5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = tf.keras.utils.to_categorical(df[\"Level\"].values, num_classes=num_classes)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['sentence'], y, test_size=0.01)"
      ],
      "metadata": {
        "id": "uevJChkgasY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "eL1YKxM3oAMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
        "\n",
        "\n",
        "def get_embeddings(sentences):\n",
        "  '''return BERT-like embeddings of input text\n",
        "  Args:\n",
        "    - sentences: list of strings\n",
        "  Output:\n",
        "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
        "  '''\n",
        "  preprocessed_text = preprocessor(sentences)\n",
        "  return encoder(preprocessed_text)['pooled_output']\n",
        "\n",
        "\n",
        "get_embeddings([\n",
        "    \"Les coûts kilométriques réels peuvent diverger sensiblement des valeurs moyennes en fonction du moyen de transport utilisé, du taux d'occupation ou du taux de remplissage, de l'infrastructure utilisée, de la topographie des lignes, du flux de trafic, etc.\"]\n",
        ")"
      ],
      "metadata": {
        "id": "tScCM2wxa0rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def balanced_recall(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced recall metric\n",
        "    recall = TP / (TP + FN)\n",
        "    \"\"\"\n",
        "    recall_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        recall_by_class = recall_by_class + recall\n",
        "    return recall_by_class / y_pred.shape[1]\n",
        "\n",
        "def balanced_precision(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced precision metric\n",
        "    precision = TP / (TP + FP)\n",
        "    \"\"\"\n",
        "    precision_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        precision_by_class = precision_by_class + precision\n",
        "    # return average balanced metric for each class\n",
        "    return precision_by_class / y_pred.shape[1]\n",
        "\n",
        "def balanced_f1_score(y_true, y_pred):\n",
        "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
        "    precision = balanced_precision(y_true, y_pred)\n",
        "    recall = balanced_recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ],
      "metadata": {
        "id": "iV1QTXL8bbeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "x = preprocessor(i)\n",
        "x = encoder(x)\n",
        "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
        "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
        "\n",
        "model = tf.keras.Model(i, x)"
      ],
      "metadata": {
        "id": "tjnbkqqncTbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      balanced_recall,\n",
        "      balanced_precision,\n",
        "      balanced_f1_score\n",
        "]\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
        "                                                      patience = 3,\n",
        "                                                      restore_best_weights = True)\n",
        "\n",
        "model.compile(optimizer = \"adam\",\n",
        "              loss = \"categorical_crossentropy\",\n",
        "              metrics = METRICS)\n",
        "\n",
        "\n",
        "\n",
        "model_fit = model.fit(x_train, \n",
        "                      y_train, \n",
        "                      epochs = n_epochs,\n",
        "                      validation_data = (x_test, y_test),\n",
        "                      callbacks = [earlystop_callback])"
      ],
      "metadata": {
        "id": "XQpHYsMRePoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unlabeled = df_test = pd.read_csv('/content/unlabelled_test_data.csv')\n",
        "def predict_class(reviews):\n",
        "  '''predict class of input text\n",
        "  Args:\n",
        "    - reviews (list of strings)\n",
        "  Output:\n",
        "    - class (list of int)\n",
        "  '''\n",
        "  return [np.argmax(pred) for pred in model.predict(reviews)]\n",
        "\n",
        "\n",
        "predict_class(df_unlabeled[\"sentence\"])"
      ],
      "metadata": {
        "id": "A0WnGRt1ui8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit = pd.DataFrame(predict_class(df_unlabeled[\"sentence\"]))\n",
        "predictions_to_submit.columns = ['difficulty']"
      ],
      "metadata": {
        "id": "LPkx17TewUWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit"
      ],
      "metadata": {
        "id": "fJaH7CFnw7Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit['difficulty'] = predictions_to_submit['difficulty'].map({0: \"A1\",\n",
        "                                    1: 'A2',\n",
        "                                    2: 'B1',\n",
        "                                    3: 'B2',\n",
        "                                    4: 'C1',\n",
        "                                    5: 'C2'})\n",
        "predictions_to_submit = predictions_to_submit.rename_axis(\"id\")\n",
        "predictions_to_submit"
      ],
      "metadata": {
        "id": "tSnJC85Uv1ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit.to_csv(\"Geneva_predictions_BertTensor4.csv\")"
      ],
      "metadata": {
        "id": "66IBZfsjx-Am"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}