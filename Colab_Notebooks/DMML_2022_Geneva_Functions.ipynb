{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn7uiib/1eShayrOeBXG4e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrian-Muino/DMML2022_Geneva/blob/main/Colab_Notebooks/DMML_2022_Geneva_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJaUIfHCbXyv",
        "outputId": "679c6588-60a1-47cf-a2c0-19918ab10703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-83e20e720e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msp_sm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fr_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_tokenizer_sm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create token object, which is used to create documents with linguistic annotations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m---> 54\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'fr_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "sp_sm = spacy.load('fr_core_news_sm')\n",
        "\n",
        "def spacy_tokenizer_sm(df):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp_sm(df)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "  \n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cleaning function\n",
        "def nltk_tokenizer(doc):\n",
        "\n",
        "    # Tokenize\n",
        "    doc = word_tokenize(doc)\n",
        "\n",
        "    # Remove uppercase and white spaces\n",
        "    doc = [word.lower().strip() for word in doc]\n",
        "    \n",
        "    return doc"
      ],
      "metadata": {
        "id": "6nTQZshncyco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load('fr_core_news_sm')\n",
        "# Create tokenizer function\n",
        "def spacy_tokenizer_2(df):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(df)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "    \n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens]\n",
        "\n",
        "    sentence=\" \".join(mytokens)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "ZggT-kajdJIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "def evaluate(y_true, pred):\n",
        "  \"\"\"\n",
        "  This method calculates the model performance metrics. Since it is a multi-class\n",
        "  classification, we decided to take the weighted average \n",
        "  for the metrics that are calculated for each class.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  report = {\n",
        "      'accuracy':accuracy_score(y_true, pred),\n",
        "      'recall':recall_score(y_true, pred, average='macro'),\n",
        "      'precision':precision_score(y_true, pred, average='macro'),\n",
        "      'f1_score':f1_score(y_true, pred, average='macro')\n",
        "  }\n",
        "\n",
        "  return report"
      ],
      "metadata": {
        "id": "ykc0l9ujda8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, pred, model):\n",
        "  \n",
        "  \"\"\"\n",
        "  A method plotting the models into a confusion matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  cf_matrix = confusion_matrix(y_test, pred)\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
        "                              display_labels=model.classes_)\n",
        "    \n",
        "  disp.plot()\n",
        "\n",
        "\n",
        "reports = {}"
      ],
      "metadata": {
        "id": "8MhjFguydeWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
        "\n",
        "def get_embeddings(sentences):\n",
        "  '''return BERT-like embeddings of input text\n",
        "  Args:\n",
        "    - sentences: list of strings\n",
        "  Output:\n",
        "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
        "  '''\n",
        "  preprocessed_text = preprocessor(sentences)\n",
        "  return encoder(preprocessed_text)['pooled_output']\n",
        "\n",
        "\n",
        "get_embeddings([\n",
        "    \"Les coûts kilométriques réels peuvent diverger sensiblement des valeurs moyennes en fonction du moyen de transport utilisé, du taux d'occupation ou du taux de remplissage, de l'infrastructure utilisée, de la topographie des lignes, du flux de trafic, etc.\"]\n",
        ")"
      ],
      "metadata": {
        "id": "vAM87YmhdjAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_recall(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced recall metric\n",
        "    recall = TP / (TP + FN)\n",
        "    \"\"\"\n",
        "    recall_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        recall_by_class = recall_by_class + recall\n",
        "    return recall_by_class / y_pred.shape[1]"
      ],
      "metadata": {
        "id": "nz5hI2OLdlod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_precision(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced precision metric\n",
        "    precision = TP / (TP + FP)\n",
        "    \"\"\"\n",
        "    precision_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        precision_by_class = precision_by_class + precision\n",
        "    # return average balanced metric for each class\n",
        "    return precision_by_class / y_pred.shape[1]"
      ],
      "metadata": {
        "id": "hZ2w6lURdnxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_f1_score(y_true, y_pred):\n",
        "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
        "    precision = balanced_precision(y_true, y_pred)\n",
        "    recall = balanced_recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ],
      "metadata": {
        "id": "5SRrVcredpRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(reviews):\n",
        "  '''predict class of input text\n",
        "  Args:\n",
        "    - reviews (list of strings)\n",
        "  Output:\n",
        "    - class (list of int)\n",
        "  '''\n",
        "  return [np.argmax(pred) for pred in model.predict(reviews)]"
      ],
      "metadata": {
        "id": "qezCQqDMdsQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
