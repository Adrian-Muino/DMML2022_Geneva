{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzg/TZT5U1liphN9HtmZO5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrian-Muino/DMML2022_Geneva/blob/main/Colab_Notebooks/DMML_Geneva_Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJaUIfHCbXyv"
      },
      "outputs": [],
      "source": [
        "def spacy_tokenizer_sm(df):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp_sm(df)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "  \n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cleaning function\n",
        "def nltk_tokenizer(doc):\n",
        "\n",
        "    # Tokenize\n",
        "    doc = word_tokenize(doc)\n",
        "\n",
        "    # Remove uppercase and white spaces\n",
        "    doc = [word.lower().strip() for word in doc]\n",
        "    \n",
        "    return doc"
      ],
      "metadata": {
        "id": "6nTQZshncyco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenizer function\n",
        "def spacy_tokenizer_2(df):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(df)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
        "    \n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens]\n",
        "\n",
        "    sentence=\" \".join(mytokens)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "ZggT-kajdJIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, pred):\n",
        "  \"\"\"\n",
        "  This method calculates the model performance metrics. Since it is a multi-class\n",
        "  classification, we decided to take the weighted average \n",
        "  for the metrics that are calculated for each class.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  report = {\n",
        "      'accuracy':accuracy_score(y_true, pred),\n",
        "      'recall':recall_score(y_true, pred, average='macro'),\n",
        "      'precision':precision_score(y_true, pred, average='macro'),\n",
        "      'f1_score':f1_score(y_true, pred, average='macro')\n",
        "  }\n",
        "\n",
        "  return report"
      ],
      "metadata": {
        "id": "ykc0l9ujda8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, pred, model):\n",
        "  \n",
        "  \"\"\"\n",
        "  A method plotting the models into a confusion matrix.\n",
        "  \"\"\"\n",
        "\n",
        "  cf_matrix = confusion_matrix(y_test, pred)\n",
        "\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
        "                              display_labels=model.classes_)\n",
        "    \n",
        "  disp.plot()\n",
        "\n",
        "\n",
        "reports = {}"
      ],
      "metadata": {
        "id": "8MhjFguydeWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
        "\n",
        "def get_embeddings(sentences):\n",
        "  '''return BERT-like embeddings of input text\n",
        "  Args:\n",
        "    - sentences: list of strings\n",
        "  Output:\n",
        "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
        "  '''\n",
        "  preprocessed_text = preprocessor(sentences)\n",
        "  return encoder(preprocessed_text)['pooled_output']\n",
        "\n",
        "\n",
        "get_embeddings([\n",
        "    \"Les coûts kilométriques réels peuvent diverger sensiblement des valeurs moyennes en fonction du moyen de transport utilisé, du taux d'occupation ou du taux de remplissage, de l'infrastructure utilisée, de la topographie des lignes, du flux de trafic, etc.\"]\n",
        ")"
      ],
      "metadata": {
        "id": "vAM87YmhdjAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_recall(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced recall metric\n",
        "    recall = TP / (TP + FN)\n",
        "    \"\"\"\n",
        "    recall_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        recall_by_class = recall_by_class + recall\n",
        "    return recall_by_class / y_pred.shape[1]"
      ],
      "metadata": {
        "id": "nz5hI2OLdlod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_precision(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced precision metric\n",
        "    precision = TP / (TP + FP)\n",
        "    \"\"\"\n",
        "    precision_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        precision_by_class = precision_by_class + precision\n",
        "    # return average balanced metric for each class\n",
        "    return precision_by_class / y_pred.shape[1]"
      ],
      "metadata": {
        "id": "hZ2w6lURdnxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_f1_score(y_true, y_pred):\n",
        "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
        "    precision = balanced_precision(y_true, y_pred)\n",
        "    recall = balanced_recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ],
      "metadata": {
        "id": "5SRrVcredpRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(reviews):\n",
        "  '''predict class of input text\n",
        "  Args:\n",
        "    - reviews (list of strings)\n",
        "  Output:\n",
        "    - class (list of int)\n",
        "  '''\n",
        "  return [np.argmax(pred) for pred in model.predict(reviews)]"
      ],
      "metadata": {
        "id": "qezCQqDMdsQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}