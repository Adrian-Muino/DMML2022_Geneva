{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adrian-Muino/DMML2022_Geneva/blob/main/Code/4.DMML_2022_Geneva_Bert%26Tensor_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A. Introduction\n"
      ],
      "metadata": {
        "id": "8aOmV2F2AH8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group project for Data Mining & Machine Learning course, at HEC UNIL 2022 (Geneva Group)\n",
        "\n",
        "This notebook is the last step we took on our journey for the competition in kaggle\n",
        "[Detecting the difficulty level of French texts](https://www.kaggle.com/competitions/detecting-french-texts-difficulty-level-2022)\n",
        "\n",
        "In this last notebook we implemented a Multi-class Text Classification using [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) and [TensorFlow](https://www.tensorflow.org/?hl=fr).\n",
        "\n",
        "BERT stands for _\"Bidirectional Encoder Representations from Transformers\"_ and is a machine learning model based on transformers. We will use TensorFlow Hub to import the Bert Model called [Keras](https://keras.io/api/models/model/) which implementation was described by [Nicolo Cosimo](https://towardsdatascience.com/multi-label-text-classification-using-bert-and-tensorflow-d2e88d8f488d).\n",
        "\n",
        "Other Bert Models could be used like [CamemBert model](https://camembert-model.fr/) or [FlauBert](https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification) but because time constraints we couldn't implement the three of them.\n",
        "\n",
        "**However, by implementing this Keras Bert Model our submission accuracy improved from 0.515 to 0.55583, our best result for the competition.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UCujSHKcAMlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Table of content"
      ],
      "metadata": {
        "id": "HY0vZvLCzCEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[A. Introduction](#scrollTo=8aOmV2F2AH8C)\n",
        "\n",
        ">>[Group project for Data Mining & Machine Learning course, at HEC UNIL 2022 (Geneva Group)](#scrollTo=UCujSHKcAMlg)\n",
        "\n",
        ">>[Table of content](#scrollTo=HY0vZvLCzCEr)\n",
        "\n",
        ">[B. Prerequisites](#scrollTo=EkKHqlA4AOTp)\n",
        "\n",
        ">>[Installations](#scrollTo=zqIpXYX7ATz5)\n",
        "\n",
        ">>[Imports](#scrollTo=MUgW7YQHAa2f)\n",
        "\n",
        ">[C. Environment set up & exploratory data analysis](#scrollTo=qi-7AUVpB_Z8)\n",
        "\n",
        ">[D. Bert & Tensor Model](#scrollTo=IAsmRfQlDRxf)\n",
        "\n",
        ">[E. Submission](#scrollTo=19wukEl_wocC)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "oQIf0F22N7kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#B. Prerequisites"
      ],
      "metadata": {
        "id": "EkKHqlA4AOTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installations"
      ],
      "metadata": {
        "id": "zqIpXYX7ATz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation\n",
        "!pip install sentence-transformers\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy link fr_core_news_sm fr\n",
        "!python -m spacy download fr_core_news_md\n",
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "HJRGo2nvAV_h",
        "outputId": "ce44c197-b793-4c8d-fdad-17767230554e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.25.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7fe990c8b5e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 176, in cb\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/usr/lib/python3.8/runpy.py\", line 144, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"/usr/lib/python3.8/runpy.py\", line 111, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/spacy/errors.py\", line 2, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/spacy/compat.py\", line 3, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/thinc/config.py\", line 4, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/thinc/types.py\", line 7, in <module>\n",
            "    from .compat import has_cupy, cupy\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/thinc/compat.py\", line 56, in <module>\n",
            "    import tensorflow.experimental.dlpack\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py\", line 37, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py\", line 42, in <module>\n",
            "    from tensorflow.python import data\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python.data import experimental\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 96, in <module>\n",
            "    from tensorflow.python.data.experimental import service\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
            "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 25, in <module>\n",
            "    from tensorflow.python.data.ops import dataset_ops\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 29, in <module>\n",
            "    from tensorflow.python.data.ops import iterator_ops\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 34, in <module>\n",
            "    from tensorflow.python.training.saver import BaseSaverBuilder\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py\", line 32, in <module>\n",
            "    from tensorflow.python.checkpoint import checkpoint_management\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/__init__.py\", line 3, in <module>\n",
            "    from tensorflow.python.checkpoint import checkpoint_view\n",
            "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 971, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 640, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "2022-12-21 19:38:01.921901: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-12-21 19:38:01.922977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-12-21 19:38:01.923003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "MUgW7YQHAa2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports the functions we use all along our projects that are in python file in our GitHub\n",
        "import requests\n",
        "url = 'https://raw.githubusercontent.com/Adrian-Muino/DMML2022_Geneva/main/Code/dmml_2022_geneva_functions.py'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('dmml_2022_geneva_functions.py', 'w') as f:\n",
        "    f.write(r.text)"
      ],
      "metadata": {
        "id": "85MRmuo2AdbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All the other imports\n",
        "import string\n",
        "import re\n",
        "\n",
        "from dmml_2022_geneva_functions import *\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier, Perceptron\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "xetHYKv1Cw01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#C. Environment set up & exploratory data analysis"
      ],
      "metadata": {
        "id": "qi-7AUVpB_Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data from our github repository\n",
        "training_data = 'https://raw.githubusercontent.com/Adrian-Muino/DMML2022_Geneva/main/Data/training_data.csv'\n",
        "unlabelled_data = 'https://raw.githubusercontent.com/Adrian-Muino/DMML2022_Geneva/main/Data/unlabelled_test_data.csv'\n",
        "\n",
        "df = df_train = pd.read_csv(training_data)\n",
        "df_unlabeled = df_test = pd.read_csv(unlabelled_data)"
      ],
      "metadata": {
        "id": "Nn-eJU-MYiLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#D. Bert & Tensor Model"
      ],
      "metadata": {
        "id": "IAsmRfQlDRxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of sentences for each category and % relative to the total.\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "num_classes = len(df[\"difficulty\"].value_counts())\n",
        "\n",
        "colors = plt.cm.Dark2(np.linspace(0, 1, num_classes))\n",
        "iter_color = iter(colors)\n",
        "\n",
        "df['difficulty'].value_counts().plot.barh(title=\"Setences by Difficulty Level\", \n",
        "                                                 ylabel=\"Level\",\n",
        "                                                 color=colors,\n",
        "                                                 figsize=(9,9))\n",
        "\n",
        "for i, v in enumerate(df['difficulty'].value_counts()):\n",
        "  c = next(iter_color)\n",
        "  plt.text(v, i,\n",
        "           \" \"+str(v)+\", \"+str(round(v*100/df.shape[0],2))+\"%\", \n",
        "           color=c, \n",
        "           va='center', \n",
        "           fontweight='bold')"
      ],
      "metadata": {
        "id": "WLm3ujBuZmZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map topic descriptions to labels\n",
        "df['Level'] = df['difficulty'].map({'A1': 0,\n",
        "                                    'A2': 1,\n",
        "                                    'B1': 2,\n",
        "                                    'B2': 3,\n",
        "                                    'C1': 4,\n",
        "                                    'C2': 5,})\n",
        "\n",
        "# drop unused column\n",
        "df = df.drop([\"difficulty\"], axis=1)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8mFW2f3-aB5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = tf.keras.utils.to_categorical(df[\"Level\"].values, num_classes=num_classes)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['sentence'], y, test_size=0.001)"
      ],
      "metadata": {
        "id": "uevJChkgasY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the model is based on the BERT transformer architecture, it will generate a pooled_output (output embedding of the entire sequence) of shape [batch size, 768], as displayed in the following example"
      ],
      "metadata": {
        "id": "fgT0zrUfv6gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
        "\n",
        "#function imported\n",
        "get_embeddings([\n",
        "    \"Les coûts kilométriques réels peuvent diverger sensiblement des valeurs moyennes en fonction du moyen de transport utilisé, du taux d'occupation ou du taux de remplissage, de l'infrastructure utilisée, de la topographie des lignes, du flux de trafic, etc.\"]\n",
        ")"
      ],
      "metadata": {
        "id": "tScCM2wxa0rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define a model as the preprocessor and encoder layers followed by a dropout and a dense layer with a softmax activation function and an output space dimensionality equal to the number of classes we want to predict:"
      ],
      "metadata": {
        "id": "XzxPopXLvin1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "x = preprocessor(i)\n",
        "x = encoder(x)\n",
        "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
        "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
        "\n",
        "model = tf.keras.Model(i, x)"
      ],
      "metadata": {
        "id": "tjnbkqqncTbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have defined the model’s structure, we can compile and fit it. We choose to train the model for 20 epochs, but we also use the EarlyStopping callback in order to monitor the validation loss during training: if the metric does not improve for at least 3 epochs (patience = 3), the training is interrupted and the weights from the epoch where the validation loss showed the best value (i.e. lowest) are restored (restore_best_weights = True):"
      ],
      "metadata": {
        "id": "Vqy_-ZdxwRuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      balanced_recall,\n",
        "      balanced_precision,\n",
        "      balanced_f1_score\n",
        "]\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
        "                                                      patience = 3,\n",
        "                                                      restore_best_weights = True)\n",
        "\n",
        "model.compile(optimizer = \"adam\",\n",
        "              loss = \"categorical_crossentropy\",\n",
        "              metrics = METRICS)\n",
        "\n",
        "\n",
        "\n",
        "model_fit = model.fit(x_train, \n",
        "                      y_train, \n",
        "                      epochs = n_epochs,\n",
        "                      validation_data = (x_test, y_test),\n",
        "                      callbacks = [earlystop_callback])"
      ],
      "metadata": {
        "id": "XQpHYsMRePoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#E. Submission"
      ],
      "metadata": {
        "id": "19wukEl_wocC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function imported\n",
        "predict_class(df_unlabeled[\"sentence\"],model)"
      ],
      "metadata": {
        "id": "A0WnGRt1ui8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit = pd.DataFrame(predict_class(df_unlabeled[\"sentence\"],model))\n",
        "predictions_to_submit.columns = ['difficulty']"
      ],
      "metadata": {
        "id": "LPkx17TewUWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit"
      ],
      "metadata": {
        "id": "fJaH7CFnw7Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit['difficulty'] = predictions_to_submit['difficulty'].map({0: \"A1\",\n",
        "                                    1: 'A2',\n",
        "                                    2: 'B1',\n",
        "                                    3: 'B2',\n",
        "                                    4: 'C1',\n",
        "                                    5: 'C2'})\n",
        "predictions_to_submit = predictions_to_submit.rename_axis(\"id\")\n",
        "predictions_to_submit"
      ],
      "metadata": {
        "id": "tSnJC85Uv1ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_to_submit.to_csv(\"Geneva_predictions_BertTensor3.csv\")"
      ],
      "metadata": {
        "id": "66IBZfsjx-Am"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
